# <b>Classify_toxic_comment</b>


We built a multi-headed model that’s capable of detecting different types of of toxicity like
threats, obscenity, insults, and identity-based hate better than Perspective’s current models. 

<b> DATA </b>

We hqave a large number of Wikipedia Comments which have been labeled by human raters for toxic behaviour. The types of toxicity are
:
* Toxic

* Severe_toxic

* Obscene

* Threat

* Insult

* Identity_hate

You must create a model which predicts a probablity of each type of toxicity for each comment.

<b> Result </b>


LSTM is used to build the above model. 


Test Accuracy A cieved: 95.9%
